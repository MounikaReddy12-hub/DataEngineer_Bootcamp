{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50a18a9b-aa35-4e07-99a4-23436a289bad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#create dataframe in realistic way\n",
    "data={\n",
    "        (\"Alice\",\"IT\",50000),\n",
    "        (\"Anurag\",\"Data Engineer\",100000),\n",
    "        (\"Bob\",\"HR\",40000),\n",
    "        (\"Stuti\",\"Recruiter\",150000),\n",
    "        (\"Ayushi\",\"HR\",30000),\n",
    "        (\"Ankur\",\"IT\",510000),\n",
    "        (\"Shruti\",\"Finance\",650000),\n",
    "        (\"Shalini\",\"Finance\",850000),\n",
    "}\n",
    "\n",
    "\n",
    "column=[\"Name\",\"Department\",\"Salary\"]\n",
    "\n",
    "df=spark.createDataFrame(data,column)\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "642136c3-217c-43e8-a5a4-21bbee114b2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df.select(\"Name\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75ceaedf-f63a-43ab-8006-24d737308c48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#transformation\n",
    "display(df.select(\"Name\",\"Salary\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "092c8527-d086-4e4f-aa66-ae1ffef662ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#apply filter on salary\n",
    "display(df.filter(df.Salary>50000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bad9233c-0a39-4d5e-8e96-75248a29af74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.groupBy(\"Department\").avg(\"Salary\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "158926d6-2f85-45e3-91f8-98e74fc40268",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#calculate min, max and avg\n",
    "from pyspark.sql.functions import min, avg, max\n",
    "\n",
    "avg_salary = df.select(avg(\"Salary\").alias(\"avg_salary\")).collect()[0][\"avg_salary\"]\n",
    "print(f\"average salary = {avg_salary}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7653e318-9a4b-4a21-ab7a-7f30cbe7799f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#calculate avg, min, max salary of each department\n",
    "\n",
    "display(df.groupBy(\"Department\").agg(\n",
    "    avg(\"Salary\").alias(\"avg_salary\"),\n",
    "    min(\"Salary\").alias(\"min_salary\"),\n",
    "    max(\"Salary\").alias(\"max_salary\")\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95420008-673e-4cfa-a314-02cdda581608",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Join two DataFrames\n",
    "dept_data = [(\"IT\", 101), (\"HR\", 102), (\"Finance\", 103)]\n",
    "dept_columns = [\"Department\", \"Dept_Code\"]\n",
    "\n",
    "df_dept = spark.createDataFrame(dept_data, dept_columns)\n",
    "\n",
    "#join\n",
    "joined_df = df.join(df_dept, on=\"Department\", how=\"inner\")\n",
    "joined_df.show()\n",
    "display(joined_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da4daa42-dc18-41a3-b2e4-1d03ab1b3856",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Transformation V/S Action \n",
    "\n",
    "\n",
    "#Transformation\n",
    "filtered = df.filter(df[\"Salary\"] > 50000)\n",
    "\n",
    "#Action\n",
    "filtered.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22635230-720a-49f1-9fd4-9b8d5b6efcb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Student Practice Assignment\n",
    "ðŸ“‚ Build a DataFrame with the following schema: Name, Department, Salary, Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58e13e6a-1845-492a-92f3-cf4463be82c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sample data\n",
    "data = [\n",
    "    (\"Anurag\", \"IT\", 70000, \"Bangalore\"),\n",
    "    (\"Priya\", \"HR\", 55000, \"Mumbai\"),\n",
    "    (\"Ravi\", \"Finance\", 65000, \"Delhi\"),\n",
    "    (\"Sneha\", \"IT\", 60000, \"Hyderabad\")\n",
    "]\n",
    "\n",
    "columns = [\"Name\", \"Department\", \"Salary\", \"Location\"]\n",
    "\n",
    "df_task = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Task 1: Filter IT employees with salary > 60K\n",
    "display(df_task.filter((df_task.Department == \"IT\") & (df_task.Salary > 60000)))\n",
    "\n",
    "# Task 2: Add column \"Hike_Amount\" as 15% of salary\n",
    "df_task=df_task.withColumn(\"Hike_Amount\", df_task.Salary * 0.15)\n",
    "display(df_task)\n",
    "\n",
    "# Task 3: Group by Department and show average salary\n",
    "display(df.groupBy(\"Department\").avg(\"Salary\"))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Pyspark_Hands-on_class_2",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
